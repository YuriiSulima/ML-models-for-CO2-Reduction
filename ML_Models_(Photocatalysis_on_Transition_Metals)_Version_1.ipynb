{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriiSulima/ML-models-for-CO2-Reduction/blob/main/ML_Models_(Photocatalysis_on_Transition_Metals)_Version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d15680",
      "metadata": {
        "tags": [],
        "id": "94d15680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "d5b02099-5220-408b-9b49-c4f40f3d0db9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-526a0e2d1c94>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_ctx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_status_ctx\u001b[0m \u001b[0;31m# line: 34\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_convert\u001b[0m \u001b[0;31m# line: 493\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/ag_ctx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_managers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_dependency_on_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malias_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamic_list_append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/utils/context_managers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAnyStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_defines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_argument_parser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Importing all the essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ce962c",
      "metadata": {
        "tags": [],
        "id": "74ce962c"
      },
      "outputs": [],
      "source": [
        "#Making a dataset from the table\n",
        "# Read the CSV file \"YS01 - ML Photocatalysis Transitional Metals.csv\" and store it in the variable 'data'\n",
        "data = pd.read_csv(\"YS01 - ML Photocatalysis Transitional Metals.csv\")\n",
        "\n",
        "# Read the CSV file \"YS01 - ML Transition Metal Updated.csv\" and store it in the variable 'metals'\n",
        "metals = pd.read_csv(\"YS01 - ML Transition Metal Updated.csv\")\n",
        "\n",
        "# Print the shape of the 'data' DataFrame\n",
        "print(data.shape)\n",
        "\n",
        "# Print the shape of the 'metals' DataFrame\n",
        "print(metals.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8adbf50e",
      "metadata": {
        "id": "8adbf50e"
      },
      "outputs": [],
      "source": [
        "#Showing general statistics about our dataset\n",
        "data.describe()\n",
        "metals.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f556af5",
      "metadata": {
        "tags": [],
        "id": "3f556af5"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of the 'data' DataFrame\n",
        "data.head()\n",
        "\n",
        "# Display the first few rows of the 'metals' DataFrame\n",
        "metals.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a6504c",
      "metadata": {
        "id": "a0a6504c"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f730d441-3ff8-48ef-aa23-d0328025aac5",
      "metadata": {
        "id": "f730d441-3ff8-48ef-aa23-d0328025aac5"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf52912",
      "metadata": {
        "tags": [],
        "id": "9bf52912"
      },
      "outputs": [],
      "source": [
        "catalyst_encoder = LabelEncoder()\n",
        "type_encoder = LabelEncoder()\n",
        "\n",
        "data.drop(['Index'],axis=1,inplace=True)\n",
        "data.drop(['pH before'],axis=1,inplace=True)\n",
        "\n",
        "data['Catalyst'] = catalyst_encoder.fit_transform(data['Catalyst'])\n",
        "groups = data['Catalyst']\n",
        "label_mapping = dict(zip(catalyst_encoder.classes_, catalyst_encoder.transform(catalyst_encoder.classes_)))\n",
        "print(label_mapping)\n",
        "\n",
        "data.drop(['Catalyst'],axis=1,inplace=True)\n",
        "data['mmol CO / g QD'] = data['mmol CO / g QD'] / data['Time ']\n",
        "data.drop(['Time '],axis=1,inplace=True)\n",
        "\n",
        "data['Type of complex (low-spin vs high-spin)'] = catalyst_encoder.fit_transform(data['Type of complex (low-spin vs high-spin)'])\n",
        "\n",
        "data['Magnetic Type'] = type_encoder.fit_transform(data['Magnetic Type'])\n",
        "label_mapping = dict(zip(type_encoder.classes_, type_encoder.transform(type_encoder.classes_)))\n",
        "print(label_mapping)\n",
        "data = data.fillna(0)\n",
        "\n",
        "features_scaler = StandardScaler()\n",
        "target_scaler = StandardScaler()\n",
        "\n",
        "data_pipeline = Pipeline([\n",
        "        ('std_scaler', features_scaler),\n",
        "    ])\n",
        "\n",
        "target_pipeline = Pipeline([\n",
        "        ('std_scaler', target_scaler),\n",
        "    ])\n",
        "\n",
        "target = data['mmol CO / g QD']\n",
        "features = data.drop('mmol CO / g QD', axis=1)\n",
        "columns_f = features.columns\n",
        "\n",
        "features = data_pipeline.fit_transform(features)\n",
        "features = pd.DataFrame(features,columns=columns_f)\n",
        "\n",
        "target = data_pipeline.fit_transform(target.to_numpy().reshape(-1,1))\n",
        "target = pd.DataFrame(target,columns=['mmol CO / g QD'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057a479d",
      "metadata": {
        "id": "057a479d"
      },
      "source": [
        "# Preprocessing Metals Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265a0994",
      "metadata": {
        "id": "265a0994"
      },
      "outputs": [],
      "source": [
        "names = metals['Element'].copy()\n",
        "\n",
        "metals['Element'] = type_encoder.fit_transform(metals['Element'])\n",
        "label_mapping = dict(zip(type_encoder.classes_, type_encoder.transform(type_encoder.classes_)))\n",
        "print(label_mapping)\n",
        "\n",
        "metals['Magnetic Type'] = type_encoder.fit_transform(metals['Magnetic Type'])\n",
        "label_mapping = dict(zip(type_encoder.classes_, type_encoder.transform(type_encoder.classes_)))\n",
        "print(label_mapping)\n",
        "\n",
        "metals = metals.fillna(0)\n",
        "\n",
        "columns_metals = metals.columns\n",
        "metals = data_pipeline.fit_transform(metals)\n",
        "metals = pd.DataFrame(metals,columns=columns_metals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b74821a",
      "metadata": {
        "id": "6b74821a"
      },
      "outputs": [],
      "source": [
        "metals.head()\n",
        "print(columns_metals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730d163d",
      "metadata": {
        "id": "730d163d"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5546b99e",
      "metadata": {
        "id": "5546b99e"
      },
      "outputs": [],
      "source": [
        "#PCA worse\n",
        "# pca = PCA(n_components=10)\n",
        "# x_train = pca.fit_transform(x_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "#T-SNE much worse\n",
        "# tsne = TSNE(n_components=3, random_state=42)\n",
        "# features = tsne.fit_transform(features)\n",
        "\n",
        "# #UMAP\n",
        "# pendigits = sklearn.datasets.load_digits()\n",
        "# reducer = umap.UMAP(n_components=2)\n",
        "# features = reducer.fit_transform(features, target)\n",
        "# plt.scatter(features[:, 0], features[:, 1],s=.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa152e6-c69f-44f0-ae20-d8c1c1977db9",
      "metadata": {
        "id": "bfa152e6-c69f-44f0-ae20-d8c1c1977db9"
      },
      "source": [
        "# Stratified Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf467148",
      "metadata": {
        "id": "cf467148"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets using GroupShuffleSplit\n",
        "# split = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "# for train_index, test_index in split.split(features, target, groups):\n",
        "#     x_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
        "#     y_train, Y_test = target.iloc[train_index], target.iloc[test_index]\n",
        "\n",
        "x_train, X_test, y_train, Y_test = train_test_split(features, target, test_size=0.3, random_state=0) #best states 5, 14, 21, 0\n",
        "index_before = Y_test.index\n",
        "Y_test = Y_test.sort_values(by='mmol CO / g QD')\n",
        "sorted_indices = Y_test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb49b39",
      "metadata": {
        "id": "0bb49b39"
      },
      "outputs": [],
      "source": [
        "data.corr()['mmol CO / g QD'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626dbe13-404a-426a-a2d1-513bcf1ebd6c",
      "metadata": {
        "tags": [],
        "id": "626dbe13-404a-426a-a2d1-513bcf1ebd6c"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e21506",
      "metadata": {
        "id": "e5e21506"
      },
      "outputs": [],
      "source": [
        "def rmse(y_actual, y_predicted):\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from math import sqrt\n",
        "    return sqrt(mean_squared_error(y_actual, y_predicted))\n",
        "def plotShap(shap_values):\n",
        "    # Calculate the mean absolute SHAP values\n",
        "    mean_abs_shap_values = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "    # Sort feature names and SHAP values together\n",
        "    sorted_indices = np.argsort(mean_abs_shap_values)\n",
        "    sorted_feature_names = columns_f[sorted_indices]\n",
        "    sorted_mean_abs_shap_values = mean_abs_shap_values[sorted_indices]\n",
        "\n",
        "    mse = mean_squared_error(Y_test, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(Y_test, pred)\n",
        "    # Plot a vertical bar chart of feature importances with values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.figtext(0.87, 0.12, f\"MSE: {mse:.5f}, RMSE: {rmse:.5f}, R2: {r2:.5f}\", horizontalalignment='right')\n",
        "    plt.barh(range(len(sorted_feature_names)), sorted_mean_abs_shap_values, color='blue')\n",
        "    plt.yticks(range(len(sorted_feature_names)), sorted_feature_names)\n",
        "    plt.xlabel('Mean Absolute SHAP Value')\n",
        "    plt.title('Feature Importances with SHAP Values')\n",
        "    # Add values next to the bars\n",
        "    for index, value in enumerate(sorted_mean_abs_shap_values):\n",
        "        plt.text(value, index, f'{value:.2f}', va='center')\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cfb878",
      "metadata": {
        "id": "54cfb878"
      },
      "outputs": [],
      "source": [
        "def best_hiper_search(features_train, target_train):\n",
        "    parametrs_DTR = {'max_depth': range (1,13, 1)}\n",
        "    parametrs_RFR = {'n_estimators': range (1, 10, 2),'max_depth': range (1,10, 1)}\n",
        "    parametrs_LR = {'fit_intercept':[True,False]}\n",
        "\n",
        "    rmse_scorer = make_scorer(rmse, greater_is_better=True)\n",
        "\n",
        "\n",
        "    grid_DTR = GridSearchCV(DecisionTreeRegressor(), parametrs_DTR, cv=5, scoring=rmse_scorer)\n",
        "    grid_RFR = GridSearchCV(RandomForestRegressor(), parametrs_RFR, cv=10, scoring=rmse_scorer)\n",
        "    grid_LR = GridSearchCV(LinearRegression(), parametrs_LR, cv=5, scoring=rmse_scorer)\n",
        "\n",
        "\n",
        "    grid_DTR.fit(features_train, target_train)\n",
        "    grid_RFR.fit(features_train, target_train)\n",
        "    grid_LR.fit(features_train, target_train)\n",
        "\n",
        "\n",
        "    print(\"DecisionTreeRegressor\", grid_DTR.scorer_, grid_DTR.best_score_, grid_DTR.best_params_)\n",
        "    print(\"RandomForestRegressor\", grid_RFR.scorer_, grid_RFR.best_score_, grid_RFR.best_params_)\n",
        "    print(\"LinearRegression\", grid_LR.scorer_, grid_LR.best_score_, grid_LR.best_params_)\n",
        "    return\n",
        "\n",
        "# Function to split, train, and evaluate the model\n",
        "def evaluate_random_state_xgb(random_state):\n",
        "    # Split the dataset using train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.3, random_state=random_state)\n",
        "\n",
        "    # Train an XGBoost model\n",
        "    model = xgb.XGBRegressor(objective='reg:squarederror', random_state=random_state)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return mse, rmse, r2\n",
        "# Function to split, train, and evaluate the model\n",
        "def evaluate_random_state_dtr(random_state):\n",
        "    # Split the dataset using train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.3, random_state=random_state)\n",
        "\n",
        "    # Train an DTR model\n",
        "    model = DecisionTreeRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return mse, rmse, r2\n",
        "def evaluate_random_state_rfr(random_state):\n",
        "    # Split the dataset using train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.3, random_state=random_state)\n",
        "\n",
        "    # Train an RFR model\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return mse, rmse, r2\n",
        "def display_shap_importance_values():\n",
        "    # Calculate the mean absolute SHAP values for each feature\n",
        "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "    columns = columns_f.to_list()\n",
        "    print(columns)\n",
        "\n",
        "    shap_importance_df = pd.DataFrame({\n",
        "        'Feature': columns,\n",
        "        'SHAP Importance': shap_importance\n",
        "    }).sort_values(by='SHAP Importance', ascending=False)\n",
        "    #print(\"Feature importance based on mean absolute SHAP values:\\n\", shap_importance_df)\n",
        "    shap_importance_df = shap_importance_df.set_index('Feature')\n",
        "    shap_importance_df = shap_importance_df.T\n",
        "    # Compute the score using a dictionary lookup for SHAP values with a default weight of 0\n",
        "    metals['Score'] = metals.apply(\n",
        "        lambda row: sum(row[feature] * shap_importance_df.get(feature, 0) for feature in columns_metals if feature != 'Element'), axis=1\n",
        "    )\n",
        "    combined_df = pd.DataFrame({\n",
        "    'Name': names,\n",
        "    'Score': metals['Score']\n",
        "    })\n",
        "    combined_df = combined_df.sort_values(by='Score')\n",
        "    print(combined_df)\n",
        "\n",
        "print(\"Selection of hyperparameters:\")\n",
        "#best_hiper_search(x_train, y_train)\n",
        "print('')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e45d5c",
      "metadata": {
        "id": "02e45d5c"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f96f05",
      "metadata": {
        "id": "01f96f05"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(fit_intercept = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9cdfae",
      "metadata": {
        "id": "9c9cdfae"
      },
      "outputs": [],
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f7a762",
      "metadata": {
        "id": "71f7a762"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97126ca6",
      "metadata": {
        "id": "97126ca6"
      },
      "outputs": [],
      "source": [
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0572946",
      "metadata": {
        "id": "f0572946"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1fedc89",
      "metadata": {
        "id": "e1fedc89"
      },
      "source": [
        "# Decision Tree Regressor\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2da630e",
      "metadata": {
        "id": "b2da630e"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#metals[\"DTR\"] ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c879ad",
      "metadata": {
        "id": "07c879ad"
      },
      "outputs": [],
      "source": [
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Compute SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Compute the base value (expected value of the model output)\n",
        "base_value = explainer.expected_value\n",
        "#shap.summary_plot(shap_values, X_test, feature_names=columns_f)\n",
        "plotShap(shap_values)\n",
        "display_shap_importance_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a39cb5-63ec-4c15-b14f-643f5b5846b4",
      "metadata": {
        "id": "e5a39cb5-63ec-4c15-b14f-643f5b5846b4"
      },
      "outputs": [],
      "source": [
        "# Try different random states and record performance\n",
        "results = []\n",
        "for random_state in range(50):  # Try 50 different random states\n",
        "    mse, rmse, r2 = evaluate_random_state_dtr(random_state)\n",
        "    results.append((random_state, mse, rmse, r2))\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Random State', 'MSE', 'RMSE', 'R²'])\n",
        "\n",
        "# Find the best random state based on RMSE\n",
        "best_random_state = results_df.loc[results_df['RMSE'].idxmin()]\n",
        "\n",
        "print(\"Best Random State:\")\n",
        "print(best_random_state)\n",
        "\n",
        "# Optionally, plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['Random State'], results_df['RMSE'], marker='o')\n",
        "plt.title('RMSE for Different Random States')\n",
        "plt.xlabel('Random State')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98f046b",
      "metadata": {
        "id": "d98f046b"
      },
      "source": [
        "# Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3feb42e2",
      "metadata": {
        "id": "3feb42e2"
      },
      "outputs": [],
      "source": [
        "model = RandomForestRegressor(max_depth = 6)\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o',markersize=12)\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o',markersize=12)\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points', fontsize = 25)\n",
        "plt.ylabel('Values', fontsize = 25)\n",
        "plt.xticks(fontsize = 22)\n",
        "plt.yticks(fontsize = 22)\n",
        "plt.title('True vs. Predicted Values', fontsize = 25)\n",
        "plt.legend(fontsize = 14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3064377",
      "metadata": {
        "id": "e3064377"
      },
      "outputs": [],
      "source": [
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Compute SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Compute the base value (expected value of the model output)\n",
        "base_value = explainer.expected_value\n",
        "#shap.summary_plot(shap_values, X_test, feature_names=columns_f)\n",
        "#shap.summary_plot(shap_values, X_test, feature_names=columns_f, plot_type=\"bar\")\n",
        "plotShap(shap_values)\n",
        "display_shap_importance_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713bc47a-dbe5-4cf2-993a-1bce7e5131d7",
      "metadata": {
        "id": "713bc47a-dbe5-4cf2-993a-1bce7e5131d7"
      },
      "outputs": [],
      "source": [
        "# Try different random states and record performance\n",
        "results = []\n",
        "for random_state in range(50):  # Try 50 different random states\n",
        "    mse, rmse, r2 = evaluate_random_state_rfr(random_state)\n",
        "    results.append((random_state, mse, rmse, r2))\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Random State', 'MSE', 'RMSE', 'R²'])\n",
        "\n",
        "# Find the best random state based on RMSE\n",
        "best_random_state = results_df.loc[results_df['RMSE'].idxmin()]\n",
        "\n",
        "print(\"Best Random State:\")\n",
        "print(best_random_state)\n",
        "\n",
        "# Optionally, plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['Random State'], results_df['RMSE'], marker='o')\n",
        "plt.title('RMSE for Different Random States')\n",
        "plt.xlabel('Random State')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18db0df7",
      "metadata": {
        "id": "18db0df7"
      },
      "source": [
        "# SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa72638f",
      "metadata": {
        "id": "fa72638f"
      },
      "outputs": [],
      "source": [
        "# model = SVR(kernel='rbf', C=100, gamma=0.1)\n",
        "# model.fit(x_train, y_train)\n",
        "# pred = model.predict(X_test)\n",
        "\n",
        "# #Sorting\n",
        "# dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "# pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "# print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "# #Calculation of Mean Absolute Error\n",
        "# mae = mean_absolute_error(Y_test, pred)\n",
        "# print('Mean Absolute Error of the model is: ', mae)\n",
        "# accuracy = model.score(X_test, Y_test)\n",
        "# print(\"Аccuracy\", \":\", accuracy)\n",
        "# r2 = r2_score(Y_test, pred)\n",
        "# print(\"R2: \", r2)\n",
        "# scores = cross_val_score(model, x_train, y_train,\n",
        "#                                 scoring=\"neg_mean_squared_error\", cv=10)\n",
        "# rmse_scores = np.sqrt(-scores)\n",
        "# display_scores(rmse_scores)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# # Plot true values\n",
        "# plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# # Plot predicted values\n",
        "# plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# # Add labels and legend\n",
        "# plt.xlabel('Data Points')\n",
        "# plt.ylabel('Values')\n",
        "# plt.title('True vs. Predicted Values')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c487d9f",
      "metadata": {
        "id": "6c487d9f"
      },
      "source": [
        "# Lasso Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e06627f",
      "metadata": {
        "id": "5e06627f"
      },
      "outputs": [],
      "source": [
        "# lasso_reg = Lasso(alpha=0.1)\n",
        "# lasso_reg.fit(x_train, y_train)\n",
        "# pred = model.predict(X_test)\n",
        "# print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "# #Calculation of Mean Absolute Error\n",
        "# mae = mean_absolute_error(Y_test, pred)\n",
        "# print('Mean Absolute Error of the model is: ', mae)\n",
        "# accuracy = model.score(X_test, Y_test)\n",
        "# print(\"Аccuracy\", \":\", accuracy)\n",
        "\n",
        "# scores = cross_val_score(model, x_train, y_train,\n",
        "#                                 scoring=\"neg_mean_squared_error\", cv=10)\n",
        "# rmse_scores = np.sqrt(-scores)\n",
        "# display_scores(rmse_scores)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# # Plot true values\n",
        "# plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# # Plot predicted values\n",
        "# plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# # Add labels and legend\n",
        "# plt.xlabel('Data Points')\n",
        "# plt.ylabel('Values')\n",
        "# plt.title('True vs. Predicted Values')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a19f237",
      "metadata": {
        "id": "5a19f237"
      },
      "source": [
        "# Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af77fdbf",
      "metadata": {
        "id": "af77fdbf"
      },
      "outputs": [],
      "source": [
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(x_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f94636",
      "metadata": {
        "id": "34f94636"
      },
      "source": [
        "# XGB Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf2dd77",
      "metadata": {
        "id": "4bf2dd77"
      },
      "outputs": [],
      "source": [
        "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab562a4c",
      "metadata": {
        "id": "ab562a4c"
      },
      "outputs": [],
      "source": [
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Compute SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Compute the base value (expected value of the model output)\n",
        "base_value = explainer.expected_value\n",
        "#shap.summary_plot(shap_values, X_test, feature_names=columns_f)\n",
        "plotShap(shap_values)\n",
        "display_shap_importance_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf071aee-2fee-4727-af9e-1dab4083abed",
      "metadata": {
        "id": "bf071aee-2fee-4727-af9e-1dab4083abed"
      },
      "outputs": [],
      "source": [
        "# Try different random states and record performance\n",
        "results = []\n",
        "for random_state in range(50):  # Try 50 different random states\n",
        "    mse, rmse, r2 = evaluate_random_state_xgb(random_state)\n",
        "    results.append((random_state, mse, rmse, r2))\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Random State', 'MSE', 'RMSE', 'R²'])\n",
        "\n",
        "# Find the best random state based on RMSE\n",
        "best_random_state = results_df.loc[results_df['RMSE'].idxmin()]\n",
        "\n",
        "print(\"Best Random State:\")\n",
        "print(best_random_state)\n",
        "\n",
        "# Optionally, plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['Random State'], results_df['RMSE'], marker='o')\n",
        "plt.title('RMSE for Different Random States')\n",
        "plt.xlabel('Random State')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be390b4",
      "metadata": {
        "id": "4be390b4"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7cfd973",
      "metadata": {
        "id": "b7cfd973"
      },
      "outputs": [],
      "source": [
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "rfr = RandomForestRegressor(random_state=42)\n",
        "voter = VotingRegressor(\n",
        "    estimators=[('dtr', dtr), ('rfr', rfr)])\n",
        "voter.fit(x_train, y_train)\n",
        "pred = voter.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037b0b89",
      "metadata": {
        "id": "037b0b89"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Bagging Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960085ef",
      "metadata": {
        "scrolled": true,
        "id": "960085ef"
      },
      "outputs": [],
      "source": [
        "bag = BaggingRegressor(\n",
        "    VotingRegressor(estimators=[('dtr', dtr), ('rfr', rfr)]), n_estimators=500,\n",
        "    max_samples=30, bootstrap=True, n_jobs=-1, random_state=42)\n",
        "bag.fit(x_train, y_train)\n",
        "pred = bag.predict(X_test)\n",
        "\n",
        "#Sorting\n",
        "dictionary = {index: prediction for index, prediction in zip(index_before, pred)}\n",
        "pred = np.array([dictionary[idx] for idx in sorted_indices])\n",
        "\n",
        "print('RMSE: ', mean_squared_error(Y_test, pred)**0.5)\n",
        "#Calculation of Mean Absolute Error\n",
        "mae = mean_absolute_error(Y_test, pred)\n",
        "print('Mean Absolute Error of the model is: ', mae)\n",
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Аccuracy\", \":\", accuracy)\n",
        "r2 = r2_score(Y_test, pred)\n",
        "print(\"R2: \", r2)\n",
        "scores = cross_val_score(model, x_train, y_train,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# # Calculate differences\n",
        "# difference = [p - t for p, t in zip(pred, Y_test)]\n",
        "\n",
        "# # Create DataFrame\n",
        "# dt = {'True Values': Y_test,\n",
        "#         'Predicted Values': pred,\n",
        "#         'Difference': difference}\n",
        "\n",
        "# df = pd.DataFrame(dt)\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002a2e98",
      "metadata": {
        "id": "002a2e98"
      },
      "source": [
        "# Perceptron (ANN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b08e61",
      "metadata": {
        "id": "a1b08e61"
      },
      "outputs": [],
      "source": [
        "#per = Perceptron(max_iter=100, random_state=42)\n",
        "#per.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b83736",
      "metadata": {
        "id": "71b83736"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930bb6b7",
      "metadata": {
        "id": "930bb6b7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(100,input_shape=(21,)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dropout(0.1),\n",
        "    Dense(100),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dropout(0.1),\n",
        "    Dense(100),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dropout(0.1),\n",
        "    Dense(3)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.fit(x_train, y_train, epochs=500, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss = model.evaluate(X_test, Y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "display_scores(rmse_scores)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot true values\n",
        "plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# Plot predicted values\n",
        "plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Values')\n",
        "plt.title('True vs. Predicted Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7b27ec",
      "metadata": {
        "id": "cc7b27ec"
      },
      "source": [
        "# RNN (Recurrent Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773152cd",
      "metadata": {
        "id": "773152cd"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.layers import SimpleRNN\n",
        "# model = Sequential([\n",
        "#     SimpleRNN(64, input_shape=(10,1)),  # RNN layer with 64 neurons\n",
        "#     Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "#     SimpleRNN(64),  # RNN layer with 64 neurons\n",
        "#     Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "#     SimpleRNN(32),  # RNN layer with 32 neurons\n",
        "#     Dropout(0.2),  # Dropout layer to prevent overfitting\n",
        "#     Dense(3)  # Output layer with no activation function for regression\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# X_train_RNN = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "# X_test_RNN = x_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "# Y_train_RNN = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
        "# Y_test_RNN = y_test.reshape((Y_test.shape[0], Y_test.shape[1], 1))\n",
        "\n",
        "# model.fit(X_train_RNN, Y_train_RNN, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# loss = model.evaluate(X_test_RNN, Y_test_RNN)\n",
        "# print(\"Test Loss:\", loss)\n",
        "\n",
        "# pred = model.predict(X_test_RNN)\n",
        "\n",
        "# rmse_scores = np.sqrt(-scores)\n",
        "# display_scores(rmse_scores)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# # Plot true values\n",
        "# plt.plot(range(len(Y_test)), Y_test, label='True Values', marker='o')\n",
        "\n",
        "# # Plot predicted values\n",
        "# plt.plot(range(len(pred)), pred, label='Predicted Values', marker='o')\n",
        "\n",
        "\n",
        "# # Add labels and legend\n",
        "# plt.xlabel('Data Points')\n",
        "# plt.ylabel('Values')\n",
        "# plt.title('True vs. Predicted Values')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e072d84",
      "metadata": {
        "id": "9e072d84"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13859de6",
      "metadata": {
        "id": "13859de6"
      },
      "outputs": [],
      "source": [
        "pred = voter.predict([[1,10,1,0,0,0,0,0,0]])\n",
        "pred = pred.reshape(1,-1)\n",
        "data_pipeline.inverse_transform(pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8f76b4",
      "metadata": {
        "id": "fa8f76b4"
      },
      "source": [
        "# RL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e35ea2",
      "metadata": {
        "id": "b9e35ea2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}